**Project Title:**
**Credit Card Fraud Detection using Machine Learning and Sampling Techniques**

**Introduction:**
Credit card fraud detection is an important problem in machine learning. In real-world data, fraudulent transactions are very few compared to normal transactions. This makes the dataset imbalanced 
and difficult for machine learning models to learn correctly. In this assignment, we handle the imbalance problem using different sampling techniques and apply multiple machine learning models.

**Dataset**

1.The credit card fraud detection dataset is imbalanced
2.Fraud transactions are very few compared to non-fraud ones

**Balancing the dataset**

1.Random Under-Sampling:It reduces the majority class and makes both classes equal.
Risk: loss of useful data

2️.Random Over-Sampling:It duplicates minority class samples and improves fraud detection.
Risk: overfitting

3.SMOTE (Synthetic Minority Over-sampling Technique):It generates synthetic fraud samples and preserves data diversity.

4.NearMiss:It selects majority samples closest to minority class and do aggressive under-sampling.
Lower performance observed

5.SMOTE-Tomek:It is combination of over-sampling + cleaning noisy samples
Produces a well-balanced and cleaner dataset

**Machine Learning Models Implemented**

The following models were trained on each of the five sampled datasets:

1.Logistic Regression
2.Decision Tree
3.Random Forest
4.K-Nearest Neighbors (KNN)
5.Support Vector Machine (SVM)

**Model-wise Best Sampling Technique**

M1 – Logistic Regression
Best accuracy:Sampling5 (SMOTETomek) -> 93.53%
Very close: Sampling3 (SMOTE) -> 93.10%
Worst performance:Sampling4 (NearMiss) -> 18.53%
Explanation:Logistic Regression benefits from oversampling techniques because they balance the class distribution without losing important data. NearMiss removes too much data, hurting performance.

M2 – Decision Tree
Best accuracy:Sampling2 (RandomOver) -> 98.71%
strong: Sampling3 (SMOTE) -> 98.71%
Worst performance:Sampling4 (NearMiss) -> 15.95%
Explanation:Decision Trees handle oversampled data well, but aggressive undersampling (NearMiss) removes critical patterns.

M3 – Random Forest
Best accuracy:Sampling2 (RandomOver) -> 99.14% , Sampling3 (SMOTE) -> 99.14%
Worst performance:Sampling4 (NearMiss) -> 55.60%
Explanation:Random Forest performs best with balanced and rich datasets, which is achieved using Random Oversampling and SMOTE.

M4 – KNN
Best accuracy:Sampling2 (RandomOver) -> 97.84%
Worst performance:Sampling4 (NearMiss) -> 39.66%
Explanation:KNN is sensitive to data distribution. Oversampling improves neighborhood representation, while undersampling degrades it.

M5 – SVM
Best accuracy:Sampling1 (RandomUnder) -> 77.16% , Sampling2 (RandomOver) → 87.50%
Worst performance:Sampling3 (SMOTE) -> 44.40%
Explanation:SVM struggles with synthetic samples generated by SMOTE and performs better with real data points.

**Conclusion**
Random Oversampling and SMOTE gave the best results overall, while NearMiss performed the worst due to excessive data loss.



